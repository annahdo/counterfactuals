{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0632f9",
   "metadata": {},
   "source": [
    "# Finding counterfactuals on the data manifold \n",
    "\n",
    "In this notebook we want to show how to find counterfactual that lie on the datamanifold and how they differ from adversarial examples that lie off data manifold.\n",
    "\n",
    "Let's assume our data lies on a one-dimensional manifold, a helix, that is embedded in 3 dimensional space. We define a simple classifier that divides the data into two classes. To approximate the data manifold we train a normalizing flow. With the classifier $f$ alone we can produce adversarial examples by doing gradient ascent in the data space $X$:\n",
    "\n",
    "$$\n",
    "x^{(t+1)} = x^{(t)} + \\lambda \\frac{\\partial f}{\\partial x}(x^{(t)})\n",
    "$$\n",
    "\n",
    "Where $\\lambda$ is the learning rate.\n",
    "The retrieved adversarial examples often lie off the datamanifold.\n",
    "\n",
    "\n",
    "In contrast, if we search for counterfactuals by doing gradient ascent in the latent space $Z$ of our normalizing flow $g$ we stay (approximately) on the data manifold:\n",
    "\n",
    "$$\n",
    "z^{(t+1)} = z^{(t)} + \\lambda \\frac{\\partial (f\\circ g)}{\\partial z}(z^{(t)})\n",
    "$$\n",
    "\n",
    "For more detail please refer to the paper: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819096e5",
   "metadata": {},
   "source": [
    "## Data distribution\n",
    "\n",
    "Let's start by defining a uniform data distribution on a helix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports and plot definitions\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Nimbus Roman\"],\n",
    "    \"mathtext.fontset\": \"cm\",\n",
    "    \"font.size\": 30,\n",
    "})\n",
    "\n",
    "label_font_size = 32\n",
    "title_font_size = 32\n",
    "\n",
    "# make directories\n",
    "directories = [\"models\", \"plots\", \"results\"]\n",
    "for d in directories:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our target distribution\n",
    "class Helix:\n",
    "\n",
    "    def __init__(self, r=1, sigma=8):\n",
    "        self.sigma = sigma\n",
    "        self.r = r\n",
    "    def sample(self, num_samples):\n",
    "        z = (torch.rand(num_samples)-0.5)*self.sigma \n",
    "\n",
    "        samples = torch.zeros([num_samples, 3])\n",
    "\n",
    "        samples[:, 0] = self.r * torch.sin(z)\n",
    "        samples[:, 1] = self.r * torch.cos(z)\n",
    "        samples[:, 2] = z\n",
    "\n",
    "        return samples\n",
    "    \n",
    "    def get_class(self, samples):\n",
    "        return (samples[:, 2]>0).to(dtype=torch.float32)\n",
    "    \n",
    "    def dist(self, sample):\n",
    "        \n",
    "        sample_np = sample.numpy()\n",
    "        \n",
    "        def f(z):\n",
    "            return np.array([self.r * np.sin(z), self.r * np.cos(z), z])\n",
    "\n",
    "\n",
    "        def objective(X):\n",
    "            return np.linalg.norm(X - sample_np)\n",
    "\n",
    "\n",
    "        def con(X):\n",
    "            z = X[2]\n",
    "            return np.linalg.norm(f(z) - X)\n",
    "\n",
    "\n",
    "        x0 = f(sample_np[2])\n",
    "\n",
    "        nlc = NonlinearConstraint(con, 0.0, 0.0)\n",
    "        X_sol = scipy.optimize.minimize(objective, x0, args=(), method='SLSQP', constraints=nlc)\n",
    "        \n",
    "        return np.linalg.norm(X_sol.x - sample_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72034977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot function\n",
    "def scatter_plot(ax, samples, title=' ', lim_min=None, lim_max=None, alpha=1.0, s=1, color=None, label=None):\n",
    "    ax.scatter(samples[:, 0], samples[:, 1], samples[:, 2], alpha=alpha, s=s, c=color, label=label)\n",
    "    ax.set_title(title, fontsize=title_font_size)\n",
    "    ax.set_xlabel(\"\\n\"+r\"$x_1$\")\n",
    "    ax.set_ylabel(\"\\n\"+r\"$x_2$\")\n",
    "    ax.set_zlabel(\"\\n\"+r\"$x_3$\")\n",
    "    if lim_min is not None and lim_max is not None:\n",
    "        ax.set_xlim(lim_min[0], lim_max[0])\n",
    "        ax.set_ylim(lim_min[1], lim_max[1])\n",
    "        ax.set_zlim(lim_min[2], lim_max[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distr = Helix()\n",
    "\n",
    "samples_target = target_distr.sample(500)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "mini = torch.min(samples_target).detach().numpy()\n",
    "maxi = torch.max(samples_target).detach().numpy()\n",
    "lim_min = [mini, mini, mini]\n",
    "lim_max = [maxi, maxi, maxi]\n",
    "\n",
    "lim_min = torch.min(samples_target, dim=0)[0].detach().numpy()\n",
    "lim_max = torch.max(samples_target, dim=0)[0].detach().numpy()\n",
    "scatter_plot(ax, samples_target.detach().numpy(), 'target', lim_min, lim_max, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5355f1",
   "metadata": {},
   "source": [
    "## Flow\n",
    "\n",
    "Now lets define a normalizing flow that we train to map samples from a multivariate standard Normal distribution to the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb665047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of latent distribution\n",
    "latent_distr = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(3), torch.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the normalizing flow\n",
    "\n",
    "nets = lambda: nn.Sequential(nn.Linear(3, 256), nn.LeakyReLU(), \n",
    "                             nn.Linear(256, 256), nn.LeakyReLU(), \n",
    "                             nn.Linear(256, 3), nn.Tanh())\n",
    "nett = lambda: nn.Sequential(nn.Linear(3, 256), nn.LeakyReLU(), \n",
    "                             nn.Linear(256, 256), nn.LeakyReLU(), \n",
    "                             nn.Linear(256, 3))\n",
    "\n",
    "\n",
    "class RealNVP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealNVP, self).__init__()\n",
    "        mask = torch.from_numpy(np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0], \n",
    "                                          [0, 0, 1], [1, 0, 0], [0, 1, 0]] * 2).astype(np.float32))\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(len(mask))])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(len(mask))])\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = z\n",
    "        for i in range(len(self.t)):\n",
    "            x_ = x * self.mask[i]\n",
    "            s = self.s[i](x_) * (1 - self.mask[i])\n",
    "            t = self.t[i](x_) * (1 - self.mask[i])\n",
    "            x = x_ + (1 - self.mask[i]) * (x * torch.exp(s) + t)\n",
    "        return x\n",
    "\n",
    "    def reverse(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in reversed(range(len(self.t))):\n",
    "            z_ = self.mask[i] * z\n",
    "            s = self.s[i](z_) * (1 - self.mask[i])\n",
    "            t = self.t[i](z_) * (1 - self.mask[i])\n",
    "            z = (1 - self.mask[i]) * (z - t) * torch.exp(-s) + z_\n",
    "            log_det_J -= s.sum(dim=1)\n",
    "            \n",
    "        return z, log_det_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to train the flow\n",
    "\n",
    "def train_flow(flow, latent_distr, target_distr, batch_size, epochs, optimizer, save_as, best_loss = np.inf):\n",
    "    avg_loss = 0\n",
    "    flow.train()\n",
    "    with tqdm(total=epochs) as progress_bar:\n",
    "        for epoch in range(epochs):\n",
    "            samples = target_distr.sample(batch_size)\n",
    "            z, log_det = flow.reverse(samples)\n",
    "            log_prob = latent_distr.log_prob(z)\n",
    "            loss = -(log_det.mean() + log_prob.mean())\n",
    "            flow.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss = moving_avg(avg_loss, loss, epoch + 1).item()\n",
    "            progress_bar.set_postfix(loss=avg_loss)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            if (epoch+1)%100==0:\n",
    "                flow.eval()\n",
    "                avg_loss_val = 0\n",
    "                for epoch_val in range(100):\n",
    "                    samples = target_distr.sample(batch_size)\n",
    "                    z, log_det = flow.reverse(samples)\n",
    "                    log_prob = latent_distr.log_prob(z)\n",
    "                    loss = -(log_det.mean() + log_prob.mean())\n",
    "                    avg_loss_val = moving_avg(avg_loss_val, loss, epoch_val + 1).item()\n",
    "\n",
    "                if avg_loss_val<best_loss:\n",
    "                    best_loss=avg_loss_val\n",
    "                    torch.save(flow, save_as)\n",
    "                flow.train()\n",
    "                    \n",
    "    flow.eval()\n",
    "    avg_loss = 0\n",
    "    with tqdm(total=100) as progress_bar:\n",
    "        for epoch in range(100):\n",
    "            samples = target_distr.sample(batch_size)\n",
    "            z, log_det = flow.reverse(samples)\n",
    "            log_prob = latent_distr.log_prob(z)\n",
    "            loss = -(log_det.mean() + log_prob.mean())\n",
    "            avg_loss = moving_avg(avg_loss, loss, epoch + 1).item()\n",
    "            progress_bar.set_postfix(loss=avg_loss)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    return 1\n",
    "\n",
    "def moving_avg(current_avg, new_value, idx):\n",
    "    return current_avg + (new_value - current_avg) / idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487bc307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train flow or load checkpoint \n",
    "\n",
    "flow = RealNVP()\n",
    "lr = 1e-4\n",
    "epochs = 5000\n",
    "batch_size = 500\n",
    "optimizer = optim.Adam(flow.parameters(), lr=lr)\n",
    "save_as = f\"models/flow.pth\"\n",
    "\n",
    "if os.path.isfile(save_as):\n",
    "    flow = torch.load(save_as)\n",
    "else:\n",
    "    train_flow(flow, latent_distr, target_distr, batch_size, epochs, optimizer, save_as)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc395dde",
   "metadata": {},
   "source": [
    "Let's check what our flow has learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90320edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1500\n",
    "latent_distr = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(3), torch.eye(3))\n",
    "torch.manual_seed(1)\n",
    "samples_latent = latent_distr.sample((num_samples,))\n",
    "samples_target = target_distr.sample(num_samples)\n",
    "samples_flow = flow.forward(latent_distr.sample((num_samples,)))\n",
    "\n",
    "tensors = torch.cat([samples_target, samples_flow], dim=0)\n",
    "torch.min(tensors, dim=0)[0].detach().numpy()\n",
    "torch.max(tensors, dim=0)[0].detach().numpy()\n",
    "lim_min = torch.min(tensors, dim=0)[0].detach().numpy()\n",
    "lim_max = torch.max(tensors, dim=0)[0].detach().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(21, 8))\n",
    "ax = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "scatter_plot(ax, samples_latent.detach().numpy(), 'latent distribution', lim_min*2, lim_max*2)\n",
    "ax = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "scatter_plot(ax, samples_target.detach().numpy(), 'target distribution', lim_min, lim_max)\n",
    "ax = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "scatter_plot(ax, samples_flow.detach().numpy(), 'learned distribution', lim_min, lim_max)\n",
    "fig.savefig(f'plots/learned_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b9a64",
   "metadata": {},
   "source": [
    "## Adding a classifier\n",
    "\n",
    "We add a classifier. For simplicity we just define points with z-coordinate smaller than zero to belong to the one class and points with z-coordinate bigger than zero to belong to the other class. \n",
    "The binary classifier is a simple neural network with one hidden layer (256 neurons) and a single output neuron. The output is scaled to lie between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25988ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier definition\n",
    "classifier = nn.Sequential(nn.Linear(3, 256), nn.ReLU(),\n",
    "                             nn.Linear(256, 1), nn.Sigmoid())\n",
    "\n",
    "\n",
    "# training function\n",
    "def train_classifier(classifier, target_distr, batch_size, epochs, optimizer, save_as):\n",
    "\n",
    "    avg_loss = 0\n",
    "    classifier.train()\n",
    "    best_loss = np.inf\n",
    "    loss_fun = torch.nn.BCELoss(reduction=\"mean\")\n",
    "    with tqdm(total=epochs) as progress_bar:\n",
    "        for epoch in range(epochs):\n",
    "            samples = target_distr.sample(batch_size)\n",
    "            target = target_distr.get_class(samples)\n",
    "            prediction = classifier.forward(samples).squeeze()\n",
    "            loss = loss_fun(prediction, target)\n",
    "            classifier.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss = moving_avg(avg_loss, loss, epoch + 1).item()\n",
    "            progress_bar.set_postfix(loss=avg_loss)\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "    torch.save(classifier, save_as)\n",
    "                    \n",
    "    flow.eval()\n",
    "    avg_loss = 0\n",
    "    with tqdm(total=100) as progress_bar:\n",
    "        for epoch in range(100):\n",
    "            samples = target_distr.sample(batch_size)\n",
    "            target = target_distr.get_class(samples)\n",
    "            prediction = classifier.forward(samples).squeeze()\n",
    "            loss = loss_fun(prediction, target)\n",
    "            classifier.zero_grad()\n",
    "            avg_loss = moving_avg(avg_loss, loss, epoch + 1).item()\n",
    "            progress_bar.set_postfix(loss=avg_loss)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    return 1        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train classifier or load checkpoint\n",
    "\n",
    "epochs = 3000\n",
    "batch_size = 500\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "save_as = f\"models/classifier.pth\"\n",
    "\n",
    "if os.path.isfile(save_as):\n",
    "    classifier = torch.load(save_as)\n",
    "else:\n",
    "    train_classifier(classifier, target_distr, batch_size, epochs, optimizer, save_as)\n",
    "    \n",
    "    \n",
    "# test clasifier on original data\n",
    "samples_manifold = target_distr.sample((500))\n",
    "predictions = classifier(samples_manifold).squeeze()\n",
    "classes = target_distr.get_class(samples_manifold).squeeze()\n",
    "correct = torch.sum(torch.round(predictions)==classes)\n",
    "print(f\"classifier accuracy on data from target distribution:\\t{correct*100/len(samples_manifold):.2f}%\")\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "mask = predictions > 0.5\n",
    "samples_manifold1 = samples_manifold[mask, :].detach().numpy()\n",
    "samples_manifold2 = samples_manifold[~mask, :].detach().numpy()\n",
    "\n",
    "scatter_plot(ax, samples_manifold1, ' ', alpha=.2, lim_min=lim_min, lim_max=lim_max, s=64, color=\"tab:orange\")\n",
    "scatter_plot(ax, samples_manifold2, 'classification of data samples', alpha=.2, lim_min=lim_min, lim_max=lim_max, s=64, color=\"gray\")\n",
    "\n",
    "# test clasifier on generated data\n",
    "z = latent_distr.sample((500,))\n",
    "samples = flow.forward(z)\n",
    "\n",
    "predictions = classifier(samples).squeeze()\n",
    "classes = target_distr.get_class(samples).squeeze()\n",
    "correct = torch.sum(torch.round(predictions)==classes)\n",
    "print(f\"classifier accuracy on data from learned distribution:\\t{correct*100/len(samples):.2f}%\")\n",
    "\n",
    "mask = predictions > 0.5\n",
    "samples_flow1 = samples[mask, :].detach().numpy()\n",
    "samples_flow2 = samples[~mask, :].detach().numpy()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "scatter_plot(ax, samples_flow1, ' ', alpha=.2, lim_min=lim_min, lim_max=lim_max, s=64, color=\"tab:orange\")\n",
    "scatter_plot(ax, samples_flow2, 'classification on generated samples', alpha=.2, lim_min=lim_min, lim_max=lim_max, s=64, color=\"gray\")\n",
    "fig.savefig(f'plots/predictions_from_classifier.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9b2bc",
   "metadata": {},
   "source": [
    "## Finding adversarial examples and counterfactuals\n",
    "\n",
    "Now that we have a classifier and a flow we can generate adversarial examples and counterfactuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for finding adversarial examples\n",
    "def conv_attack(classifier, x_org, target, steps, lr=1e-3):\n",
    "    \n",
    "    x = x_org.clone()\n",
    "    x.requires_grad = True\n",
    "    \n",
    "    optimizer = optim.Adam(params=[x], lr=lr)\n",
    "    xs = []\n",
    "    \n",
    "    for i in range(steps):\n",
    "        xs.append(x.detach().clone())\n",
    "        optimizer.zero_grad()\n",
    "        prediction = classifier.forward(x).squeeze()\n",
    "        \n",
    "        if target >= 0.5 and prediction >= target:\n",
    "            break\n",
    "        if target < 0.5 and prediction <= target:\n",
    "            break\n",
    "            \n",
    "        loss = (prediction-target)**2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return xs\n",
    "\n",
    "# function for finding counterfactuals on the data manifold\n",
    "def z_attack(classifier, flow, x_org, target, steps, lr=1e-3):\n",
    "    \n",
    "    z, _ = flow.reverse(x_org)\n",
    "    z = z.detach()\n",
    "    z.requires_grad = True\n",
    "    xs = []\n",
    "\n",
    "    optimizer = optim.Adam(params=[z], lr=lr)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        x = flow.forward(z)\n",
    "        xs.append(x.detach().clone())\n",
    "        optimizer.zero_grad()\n",
    "        prediction = classifier.forward(x).squeeze()\n",
    "        \n",
    "        if target >= 0.5 and prediction >= target:\n",
    "            break\n",
    "        if target < 0.5 and prediction <= target:\n",
    "            break\n",
    "            \n",
    "        loss = (prediction-target)**2\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "\n",
    "    return xs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4122d2",
   "metadata": {},
   "source": [
    "We do gradient ascent steps until we reach a target value of $f(x^\\prime)\\geq0.9$, if the original data point was predicted to belong to class 0 $(f(x)<0.5)$, and $f(x^\\prime)\\leq0.1$ if the original data point was predicted to belong to class 1 $(f(x)\\leq0.5)$.\n",
    "We plot a few steps for the attack in $X$ as well as in $Z$ space to see how the coordinates of the modified points $x^\\prime$ and $g(z^\\prime)$ change during the attack. As expected the attack in $X$ quickly leads off the data manifold while the attack in $Z$ stays on the data manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv attack for one sample\n",
    "\n",
    "# define original datapoint on Helix\n",
    "z_value=2\n",
    "x_org = torch.FloatTensor([[ np.sin(z_value), np.cos(z_value),  z_value]])\n",
    "pred_x_org = classifier.forward(x_org).squeeze().item()\n",
    "print(f\"x_org:                 {x_org}\")\n",
    "print(f\"prediction x_org:      {pred_x_org:.4f}\\n\")\n",
    "\n",
    "steps = 1000\n",
    "target = 0.9 if pred_x_org<0.5 else 0.1\n",
    "    \n",
    "# conventional adversarial attack\n",
    "xs_conv = conv_attack(classifier, x_org, target, steps, lr=2e-2)\n",
    "print(f\"conv_attack steps:     {len(xs_conv)}\")\n",
    "print(f\"x_conv:                {xs_conv[-1]}\")\n",
    "print(f\"prediction x_conv:     {classifier.forward(torch.Tensor(xs_conv[-1])).squeeze().item():.4f}\")\n",
    "print(f\"dist to manifold:      {target_distr.dist(xs_conv[-1].squeeze()):.4f}\\n\")\n",
    "\n",
    "xs_conv = np.concatenate(xs_conv)\n",
    "\n",
    "# attack in the latent space of the flow\n",
    "xs_z = z_attack(classifier, flow, x_org, target, steps, lr=2e-2)\n",
    "print(f\"z-attack steps:        {len(xs_z)}\")\n",
    "print(f\"x_z:                   {xs_z[-1]}\")\n",
    "print(f\"prediction x_z:        {classifier.forward(xs_z[-1]).squeeze().item():.4f}\")\n",
    "print(f\"dist to manifold:      {target_distr.dist(xs_z[-1].squeeze()):.4f}\")\n",
    "\n",
    "xs_z = np.concatenate(xs_z)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "    \n",
    "scatter_plot(ax, samples_flow1[:150], ' ', alpha=.2, lim_min=lim_min, lim_max=lim_max, s=64, color=\"tab:orange\")\n",
    "scatter_plot(ax, samples_flow2[:150], r'Attacks in $\\mathcal{X}$ and $\\mathcal{Z}$', alpha=.2, lim_min=lim_min, lim_max=lim_max, s=64, color=\"gray\")\n",
    "    \n",
    "ax.plot(xs_conv[:,0], xs_conv[:,1], xs_conv[:,2], lw=3, color=\"tab:red\", zorder=15, label='grad asc in $\\\\mathcal{X}$')\n",
    "ax.plot(xs_z[:,0], xs_z[:,1], xs_z[:,2], lw=3, color=\"tab:green\", zorder=15, label='grad asc in $\\\\mathcal{Z}$')\n",
    "ax.plot(x_org[0,0], x_org[0,1], x_org[0,2], color=\"tab:blue\", marker='x', linestyle=\"None\", zorder=15, markersize=30, markeredgewidth=4, label=r'$x$')\n",
    "ax.plot(xs_conv[-1,0], xs_conv[-1,1], xs_conv[-1,2], color=\"tab:red\", linestyle=\"None\", marker='x', zorder=10, markersize=17, markeredgewidth=3, label=r'$x^\\prime$')\n",
    "ax.plot(xs_z[-1,0], xs_z[-1,1], xs_z[-1,2], color=\"tab:green\", marker='x', linestyle=\"None\", zorder=10, markersize=17, markeredgewidth=3, label=r'$g(z^\\prime)$')\n",
    "\n",
    "ax.legend(fontsize=label_font_size, bbox_to_anchor=[1.1,0.9])\n",
    "ax.set_xticks([-1, 0, 1])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "ax.set_zticks([-3, 0, 3])\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "for t in ax.zaxis.get_major_ticks(): t.label.set_fontsize(30)\n",
    "\n",
    "plt.subplots_adjust(left=-0.3, bottom=0.01, right=0.95, top=0.99, wspace=0.2, hspace=0.05)\n",
    "fig.savefig(f'plots/adv_attack.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e0e1f",
   "metadata": {},
   "source": [
    "### Statistical evaluation\n",
    "\n",
    "Lets do a few more attacks and look at some statistics regarding the retrieved adversarial examples and counterfactuals. As we have an analytical definition of the helix and thus the data manifold we can easily calculate the distance from a found adversarial example or counterfactual to the data manifold. When we average over these distances we can see that they are on average much larger for adversarial examples found in $X$ than for counterfactuals found in $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv attacks statistics\n",
    "num_attacks = 100\n",
    "\n",
    "dists_conv = np.zeros(num_attacks)\n",
    "dists_conv = np.empty((num_attacks,))\n",
    "dists_conv[:] = np.nan\n",
    "\n",
    "dists_z = np.zeros(num_attacks)\n",
    "dists_z = np.empty((num_attacks,))\n",
    "dists_z[:] = np.nan\n",
    "\n",
    "\n",
    "dists_conv_file = f\"results/dists_conv.txt\"\n",
    "dists_z_file = f\"results/dists_z.txt\"\n",
    "\n",
    "with tqdm(total=num_attacks) as progress_bar:\n",
    "    for i in range(num_attacks):\n",
    "\n",
    "\n",
    "        torch.manual_seed(i)\n",
    "        x_org = target_distr.sample(1)\n",
    "        pred_x_org = classifier.forward(x_org).squeeze().item()\n",
    "\n",
    "        target = 0.9 if pred_x_org < 0.5 else 0.1\n",
    "\n",
    "        # conv attack\n",
    "        xs_conv = conv_attack(classifier, x_org, target, steps=1000, lr=3e-2)\n",
    "        # save if attack was successful\n",
    "        if (target==0.9 and classifier(xs_conv[-1])>=0.9) or (target==0.1 and classifier(xs_conv[-1])<=0.1):\n",
    "            dists_conv[i] = target_distr.dist(xs_conv[-1].squeeze())\n",
    "\n",
    "        # z attack\n",
    "        xs_z = z_attack(classifier, flow, x_org, target, steps=1000, lr=3e-2)\n",
    "        # save if attack was successful\n",
    "        if (target==0.9 and classifier(xs_z[-1])>=0.9) or (target==0.1 and classifier(xs_z[-1])<=0.1):\n",
    "            dists_z[i] = target_distr.dist(xs_z[-1].squeeze())\n",
    "            \n",
    "        progress_bar.update()\n",
    "\n",
    "np.savetxt(dists_conv_file, dists_conv)\n",
    "np.savetxt(dists_z_file, dists_z)\n",
    "    \n",
    "dists_conv = dists_conv[~np.isnan(dists_conv)]\n",
    "dists_z = dists_z[~np.isnan(dists_z)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e09704",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"successful attacks:\")\n",
    "print(f\"             in X: {len(dists_conv)}/{num_attacks}\")\n",
    "print(f\"             in Z: {len(dists_z)}/{num_attacks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574eea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dists_conv = np.loadtxt(dists_conv_file)\n",
    "dists_z = np.loadtxt(dists_z_file)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.grid()\n",
    "c = \"k\"\n",
    "c2=\"lightblue\"\n",
    "plt.boxplot([dists_conv, dists_z], positions=[1,2], widths = .6, notch=False, patch_artist=True,\n",
    "                boxprops=dict(facecolor=c2, color=c),\n",
    "                capprops=dict(color=c),\n",
    "                whiskerprops=dict(color=c),\n",
    "                flierprops=dict(color=c, markeredgecolor=c),\n",
    "                medianprops=dict(color=\"red\", lw=2),\n",
    "                )\n",
    "\n",
    "ax.set_ylabel(f\"distance to helix\", fontsize=label_font_size)\n",
    "ax.set_xlabel(f\"gradient ascent\", fontsize=label_font_size)\n",
    "ax.set_xticklabels(['in $\\mathcal{X}$', 'in $\\mathcal{Z}$'])\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.subplots_adjust(left=0.2, bottom=0.2, right=0.95, top=0.95, wspace=0.2, hspace=0.05)\n",
    "fig.savefig(f'plots/distances.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
